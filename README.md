# multimodal_routing
[UPDATE COMING SOON] Repository for the paper Interpretable Multimodal Routing for Human Multimodal Language.

### Prerequisites
- Python 3.6
- [Pytorch (>=1.2.0)](https://pytorch.org/) (performance might vary in different versions)
- CUDA 10.0 or above

### Datasets

Data files (containing processed MOSI, MOSEI and IEMOCAP datasets) can be downloaded from [here](https://www.dropbox.com/sh/hyzpgx1hp9nj37s/AAB7FhBqJOFDw2hEyvv2ZXHxa?dl=0).

To retrieve the meta information and the raw data, please refer to the [SDK for these datasets](https://github.com/A2Zadeh/CMU-MultimodalSDK).

### Commands
To run the program which gives computational results like accuracy and F1 scores, 

```bash run.sh```

Analysis of interpretability in jupyter notebooks will come soon.

### Acknowledgement
Some portion of the code were adapted from the [multimodal_transformer](https://github.com/yaohungt/Multimodal-Transformer) repo.

